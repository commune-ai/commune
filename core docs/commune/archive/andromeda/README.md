# [Support Our Training Strategy For Andromeda!](https://www.figma.com/file/pfaU8Nhyw0EdXuT6z4Hutw/Andromeda-Strategy?type=whiteboard&node-id=0%3A1&t=Tub1wIzaPAXt2i86-1)

# Agora
Agora is an innovative, open-source, Multi-Modality AI Research Organization committed to advancing humanity through AI. 

With the readiness of our revolutionary language model, Andromeda, we're actively looking for cloud or grant providers to assist with its training. Following training, we intend to release the model as open source. For more information or if you are interested in partnering with us, please contact: `kye@apac.ai`.

![Agora Banner](agora-banner.png)

[Join us on Discord and contribute to the Andromeda project and over 40 others.](https://discord.gg/qUtxnK2NMf)

# Andromeda: An Ultra-Fast State-Of-The-Art Language Model üöÄüåå

![Andromeda: The Next Generation Open Source Language Model](/andromeda-banner.png)

Andromeda is an ultra-intelligent and ultra-fast language model that aims to redefine the possibilities of natural language understanding. Its design emphasises performance and efficiency, making use of advanced AI techniques to rival high-profile alternatives such as OpenAI's GPT-4 and PALM.

---

# Installation & Usage
Andromeda can be used in two ways. The first method is by installing it directly using pip via `pip install Andromeda-llm`, and the second method is by cloning the repository with `git clone`. For an elaborate guide on usage and training, refer to our [training SOP](DOCs/TRAINING.md) and [documentation](https://github.com/kyegomez/Andromeda/blob/master/DOCs/DOCUMENTATION.md).

# Andromeda Model Architecture üß†üîß
Andromeda consists of a Transformer Wrapper architected through various cutting-edge features. It includes alibi positional bias, rotary position encoding, flash attention, and deep normalization support. For more information on these features and their advantages, visit our documentation.

## Roadmap üó∫Ô∏èüìç

1. **Training Phase**: Train Andromeda on an extensive dataset to achieve exceptional performance across various natural language processing tasks.
2. **Capitalize on Strong Inference Infrastructure**: Construct a comprehensive infrastructure that empowers techniques like model quantization, distillation, and efficient serving frameworks.
3. **Ongoing Improvement**: Continuously fine-tune Andromeda on a variety of data sources for superior adaptability.
4. **Foster Community-driven Development**: Encourage open-source contributions to improve pre-processing techniques, advance training practices, and pioneer novel use cases.

# Why we built Andromeda? üå†üí°
Andromeda is an attempt to create a language model capable of potentially fine tuning with 100k+ token sequence length and empower state-of-the-art language comprehension leveraging AI.

# Andromeda Principles
Our principles guiding Andromeda's development include:

* **Efficiency**: Incorporating cutting-edge optimization techniques for efficient training and inference.
* **Flexibility**: Offering a modular design that is adaptable to various tasks and domains.
* **Scalability**: Designing an architecture that can scale up with expanding computational resources and data sizes.
* **Community-driven**: Fostering a collaborative environment for improvements and innovations through open-source contributions.

Join us on this exciting journey to revolutionize the world of AI-powered language modeling! üöÄüåü

## To-Do:
* Build the most reliable and validated training and finetuning strategy. 
* Integrate Token Monster.
* Prioritize 200k instruction sample lengths for Tool API Calls.
* Train on the Gorilla Dataset.
* Establish fine-tuning scripts using tactics including quantization, 4-bit precision, and tactics like LoRA.
* Develop reinforcement scripts to train on rewards from both human and agent feedback.