# Simple example of a training script.
pretrained_model_name_or_path: null
revision: null
tokenizer_name: null
instance_data_dir: null
class_data_dir: null
instance_prompt: null
class_prompt: null
validation_prompt: null
num_validation_images: 4
validation_epochs: 50
with_prior_preservation: false
prior_loss_weight: 1.0
num_class_images: 100
output_dir: lora-dreambooth-model
seed: null
resolution: 512
center_crop: false
train_text_encoder: true
train_batch_size: 4
sample_batch_size: 4
num_train_epochs: 1
max_train_steps: null
checkpointing_steps: 500
checkpoints_total_limit: null
resume_from_checkpoint: null
gradient_accumulation_steps: 1
gradient_checkpointing: true
learning_rate: 5e-4
scale_lr: false
lr_scheduler: constant
lr_warmup_steps: 500
lr_num_cycles: 1
lr_power: 1.0
dataloader_num_workers: 0
use_8bit_adam: true
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01
adam_epsilon: 1e-08
max_grad_norm: 1.0
push_to_hub: true
hub_token: null
hub_model_id: null
logging_dir: logs
allow_tf32: true
report_to: tensorboard
mixed_precision: no
prior_generation_precision: no
local_rank: -1
enable_xformers_memory_efficient_attention: true
pre_compute_text_embeddings: true
tokenizer_max_length: null
text_encoder_use_attention_mask: true
