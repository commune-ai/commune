attention_probs_dropout_prob: 0.1
classifier_dropout : null
embedding_size: 128
hidden_act": "gelu"
hidden_dropout_prob: 0.1
hidden_size: 256
initializer_range: 0.02
intermediate_size: 1024
layer_norm_eps: 1e-12
max_position_embeddings: 512
model_type: "electra"
num_attention_heads: 4
num_hidden_layers: 12
pad_token_id: 0
position_embedding_type: "absolute"
summary_activation: "gelu"
summary_last_dropout: 0.1
summary_type: "first"
summary_use_proj: true
transformers_version: "4.20.1"
type_vocab_size: 2
use_cache: true
vocab_size: 30522