device: cuda
device_map: auto
epoch_length: 100
finetune:
  num_layers: 4
load: false
model:
  _name_or_path: EleutherAI/gpt-neo-125m
  activation_function: gelu_new
  architectures:
  - GPTNeoForCausalLM
  attention_dropout: 0
  attention_layers:
  - global
  - local
  - global
  - local
  - global
  - local
  - global
  - local
  - global
  - local
  - global
  - local
  attention_types:
  - - - global
      - local
    - 6
  bos_token_id: 50256
  embed_dropout: 0
  eos_token_id: 50256
  gradient_checkpointing: false
  hidden_size: 768
  initializer_range: 0.02
  intermediate_size: null
  layer_norm_epsilon: 1.0e-05
  max_position_embeddings: 2048
  model_name: gpt125m
  model_path: EleutherAI/gpt-neo-125m
  model_type: gpt_neo
  num_heads: 12
  num_layers: 12
  resid_dropout: 0
  summary_activation: null
  summary_first_dropout: 0.1
  summary_proj_to_labels: true
  summary_type: cls_index
  summary_use_proj: true
  transformers_version: 4.28.1
  use_cache: true
  vocab_size: 50257
  window_size: 256
optimizer:
  lr: 1.0e-05
  module: torch.optim.Adam
stats: {}
tag: null
test: true
tokenizer: EleutherAI/gpt-neo-125m
